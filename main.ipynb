{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "authentic-ethernet",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "disciplinary-month",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import connect, OperationalError, errorcodes, errors\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats as kl\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-scheduling",
   "metadata": {},
   "source": [
    "## Setting up ENV and GLOBAL variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "genetic-workstation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to database successful\n"
     ]
    }
   ],
   "source": [
    "dbname = \"census\"\n",
    "user = \"envy\"\n",
    "host = \"localhost\"\n",
    "password = \"\"\n",
    "db_relation = \"adult\"\n",
    "# reference_dataset = db_relation\n",
    "reference_dataset = \"reference_dataset\"\n",
    "target_dataset = \"target_dataset\"\n",
    "\n",
    "# All\n",
    "# measure_attributes=['age','fnlwgt', 'education_num','capital_gain','capital_loss','hours_per_week']\n",
    "# groupby_attributes=['workclass','education','occupation','relationship','race','sex','native_country','salary']\n",
    "# aggregate_functions=['sum','avg','max','min','count']\n",
    "\n",
    "# Experiments chosing only interesting attributes\n",
    "measure_attributes=['age','capital_gain','capital_loss','hours_per_week']\n",
    "groupby_attributes=['workclass','education','occupation','relationship','race','sex','native_country','salary']\n",
    "aggregate_functions=['avg','min','count']\n",
    "\n",
    "#Connecting with local db\n",
    "try:\n",
    "    conn = psycopg2.connect(f\"dbname='{dbname}' user='{user}' host='{host}' password='{password}'\")\n",
    "    cur = conn.cursor()\n",
    "    print (\"Connection to database successful\")\n",
    "except OperationalError as err:\n",
    "    print_psycopg2_exception(err)\n",
    "    conn = None\n",
    "\n",
    "#To get top k utility views\n",
    "K = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-collaboration",
   "metadata": {},
   "source": [
    "## Util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "global-daniel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_psycopg2_exception(err):\n",
    "    # get details about the exception\n",
    "    err_type, err_obj, traceback = sys.exc_info()\n",
    "\n",
    "    # get the line number when exception occured\n",
    "    line_num = traceback.tb_lineno\n",
    "    \n",
    "    # print the connect() error\n",
    "    print (f\"\"\"\\npsycopg2 ERROR: {err} on linu number: {line_num}\"\"\")\n",
    "    print (f\"\"\"psycopg2 traceback: {traceback} -- type: {err_type}\"\"\")\n",
    "\n",
    "    # psycopg2 extensions.Diagnostics object attribute\n",
    "    print (f\"\"\"\\nextensions.Diagnostics: {err.diag}\"\"\")\n",
    "\n",
    "    # print the pgcode and pgerror exceptions\n",
    "    print (f\"\"\"pgerror: {err.pgerror}\"\"\")\n",
    "    print (f\"\"\"pgcode: {err.pgcode}\"\"\", \"\\n\")\n",
    "    \n",
    "def get_cursor(conn):\n",
    "    cur = conn.cursor()\n",
    "    return cur\n",
    "\n",
    "def get_all_views(groupby_attributes, measure_attributes, aggregate_functions):\n",
    "    views = np.array(np.meshgrid(groupby_attributes, measure_attributes, aggregate_functions)).T.reshape(-1,3)\n",
    "    return views\n",
    "\n",
    "def get_aggregate_function_measure_attribute_combinations(measure_attributes, aggregate_functions):\n",
    "    combinations = np.array(np.meshgrid(measure_attributes, aggregate_functions)).T.reshape(-1,2)\n",
    "    return combinations\n",
    "\n",
    "def get_combined_aggregate_query(groupby_attribute, measure_attributes, aggregate_functions, target_relation, reference_relation):\n",
    "    aggregate_function_measure_attribute_combinations = get_aggregate_function_measure_attribute_combinations(measure_attributes, aggregate_functions)\n",
    "    aggregate_function_measure_combined_string = ','.join([f'{combination[1]}({combination[0]})' for combination in aggregate_function_measure_attribute_combinations])\n",
    "    target_query, reference_query = combined_aggregate_query_generator(groupby_attribute, aggregate_function_measure_combined_string, target_relation, reference_relation)    \n",
    "    return target_query, reference_query\n",
    "\n",
    "def combined_aggregate_query_generator(groupby_attribute, aggregate_function_measure_combined_string, target_relation, reference_relation):\n",
    "    target_query = f\"\"\"SELECT {groupby_attribute}, {aggregate_function_measure_combined_string} FROM {target_relation} GROUP BY {groupby_attribute}\"\"\"\n",
    "    reference_query = f\"\"\"SELECT {groupby_attribute}, {aggregate_function_measure_combined_string} FROM {reference_relation} GROUP BY {groupby_attribute}\"\"\"\n",
    "    return target_query, reference_query\n",
    "    \n",
    "def basic_query_generator(groupby_attribute, measure_attribute, aggregate_function, target_relation, reference_relation):\n",
    "    target_query = f\"\"\"SELECT {groupby_attribute}, {aggregate_function}({measure_attribute}) FROM {target_relation} GROUP BY {groupby_attribute}\"\"\"\n",
    "    reference_query = f\"\"\"SELECT {groupby_attribute}, {aggregate_function}({measure_attribute}) FROM {reference_relation} GROUP BY {groupby_attribute}\"\"\"\n",
    "    return target_query, reference_query\n",
    "\n",
    "def phased_query_generator(groupby_attribute, measure_attribute, aggregate_function, target_relation, reference_relation, target_boundary, reference_boundary):\n",
    "    target_query = f\"\"\"SELECT {groupby_attribute}, {aggregate_function}({measure_attribute}) \n",
    "                        FROM (SELECT * from {target_relation} offset {target_boundary[0]} row fetch next {target_boundary[1]} rows only) as X  \n",
    "                        GROUP BY {groupby_attribute}\"\"\"\n",
    "    \n",
    "    reference_query = f\"\"\"SELECT {groupby_attribute}, {aggregate_function}({measure_attribute}) \n",
    "                        FROM (SELECT * from {reference_relation} offset {reference_boundary[0]} row fetch next {reference_boundary[1]} rows only) as X  \n",
    "                        GROUP BY {groupby_attribute}\"\"\"\n",
    "    \n",
    "    return target_query, reference_query\n",
    "\n",
    "def get_phased_combined_aggregate_query(groupby_attribute, measure_attribute_aggregate_function_combinations, target_relation, reference_relation, target_boundary, reference_boundary):\n",
    "    aggregate_function_measure_combined_string = ','.join([f'{combination[1]}({combination[0]})' for combination in measure_attribute_aggregate_function_combinations])\n",
    "    target_query, reference_query = phased_combined_query_generator(groupby_attribute, aggregate_function_measure_combined_string, target_relation, reference_relation, target_boundary, reference_boundary)\n",
    "    return target_query, reference_query\n",
    "\n",
    "def phased_combined_query_generator(groupby_attribute, aggregate_function_measure_combined_string, target_relation, reference_relation, target_boundary, reference_boundary):\n",
    "    target_query = f\"\"\"SELECT {groupby_attribute}, {aggregate_function_measure_combined_string} \n",
    "                        FROM (SELECT * from {target_relation} offset {target_boundary[0]} row fetch next {target_boundary[1]} rows only) as X  \n",
    "                        GROUP BY {groupby_attribute}\"\"\"\n",
    "    \n",
    "    reference_query = f\"\"\"SELECT {groupby_attribute}, {aggregate_function_measure_combined_string} \n",
    "                        FROM (SELECT * from {reference_relation} offset {reference_boundary[0]} row fetch next {reference_boundary[1]} rows only) as X  \n",
    "                        GROUP BY {groupby_attribute}\"\"\"\n",
    "    \n",
    "    return target_query, reference_query\n",
    "    \n",
    "\n",
    "def execute_get_query(cursor, query):\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "    except Exception as err:\n",
    "        print_psycopg2_exception(err)\n",
    "        conn = None\n",
    "        \n",
    "    rows = cursor.fetchall()\n",
    "    return rows\n",
    "\n",
    "def get_key_based_values(target_result, reference_result):\n",
    "    key_set = set()\n",
    "    target_result_dict = {}\n",
    "    reference_result_dict = {}\n",
    "    final_target_values = []\n",
    "    final_reference_values = []\n",
    "    \n",
    "    for key, value in target_result:\n",
    "        key_set.add(key)\n",
    "        target_result_dict[key] = float(value)\n",
    "        \n",
    "    for key, value in reference_result:\n",
    "        key_set.add(key)\n",
    "        reference_result_dict[key] = float(value)\n",
    "        \n",
    "    for key in key_set:\n",
    "        final_target_values.append(target_result_dict.get(key, float(10e-20)) if target_result_dict.get(key, float(10e-20)) != 0.0 else float(10e-20))\n",
    "        final_reference_values.append(reference_result_dict.get(key, float(10e-20)) if reference_result_dict.get(key, float(10e-20)) != 0.0 else float(10e-20))\n",
    "        \n",
    "    return final_target_values, final_reference_values\n",
    "    \n",
    "def get_valid_gropby_value(group_by_value):\n",
    "    if group_by_value == 0.0:\n",
    "        group_by_value = float(10e-20)\n",
    "    return float(group_by_value)\n",
    "    \n",
    "def get_aggregate_function_measure_attribute_based_values(target_result, reference_result, aggregate_function_measure_attribute_combinations):\n",
    "    \"\"\"\n",
    "    query example: SELECT workclass, avg(age),count(age),avg(capital_gain),count(capital_gain),avg(hours_per_week),count(hours_per_week) FROM target_dataset GROUP BY workclass\n",
    "    target_result example:\n",
    "    [('Self-emp-not-inc', Decimal('43.0000000000000000'), 1, Decimal('0E-20'), 1, Decimal('45.0000000000000000'), 1), \n",
    "    ('Private', Decimal('48.3333333333333333'), 3, Decimal('0E-20'), 3, Decimal('33.3333333333333333'), 3)]\n",
    "    \"\"\"\n",
    "    key_set = set()\n",
    "    target_result_dict = {}\n",
    "    reference_result_dict = {}\n",
    "    final_target_values = []\n",
    "    final_reference_values = []\n",
    "    no_combinations = len(aggregate_function_measure_attribute_combinations)\n",
    "    combination_value_dict = {}\n",
    "    \n",
    "    for values in target_result:\n",
    "        key_set.add(values[0])\n",
    "        target_result_dict[values[0]] = list(map(get_valid_gropby_value, values[1:]))\n",
    "        \n",
    "    for values in reference_result:\n",
    "        key_set.add(values[0])\n",
    "        reference_result_dict[values[0]] = list(map(get_valid_gropby_value, values[1:]))\n",
    "        \n",
    "    for i, combination in enumerate(aggregate_function_measure_attribute_combinations):\n",
    "        combination = tuple(combination)\n",
    "        combination_value_dict[combination] = {\"target_values\":[], \"reference_values\":[]}\n",
    "        for key in key_set:\n",
    "            target_value = target_result_dict.get(key, [float(10e-20)]*no_combinations)[i]\n",
    "            reference_value = reference_result_dict.get(key, [float(10e-20)]*no_combinations)[i]\n",
    "            \n",
    "            combination_value_dict[combination][\"target_values\"].append(target_value)\n",
    "            combination_value_dict[combination][\"reference_values\"].append(reference_value)\n",
    "            \n",
    "    return combination_value_dict\n",
    "\n",
    "def get_aggregate_function_measure_attribute_based_KL_divergence(aggregate_function_measure_attribute_based_values):\n",
    "    kl_divergence_dict = {}\n",
    "    for aggregate_function_measure_attribute_pair in aggregate_function_measure_attribute_based_values:\n",
    "        target_vector = aggregate_function_measure_attribute_based_values[aggregate_function_measure_attribute_pair]['target_values']\n",
    "        reference_vector = aggregate_function_measure_attribute_based_values[aggregate_function_measure_attribute_pair]['reference_values']\n",
    "        kl_divergence_dict[aggregate_function_measure_attribute_pair] = calculate_kl_divergence(target_vector, reference_vector)\n",
    "        \n",
    "    return kl_divergence_dict\n",
    "        \n",
    "def calculate_kl_divergence(vector1, vector2):\n",
    "    #print(\"With manual code\", sum(vector1[i] * math.log2(vector1[i]/vector2[i]) for i in range(len(vector1))))\n",
    "    #print(\"With kl library\", kl.entropy(vector1,vector2))\n",
    "    return sum(vector1[i] * math.log2(vector1[i]/vector2[i]) for i in range(len(vector1)))\n",
    "#     return kl.entropy(vector1,vector2)\n",
    "\n",
    "def basic_get_top_k_utility_views(kl_divergence_view_mapping_list, k):\n",
    "    top_k_views = [view for kl_divergence, view in sorted(kl_divergence_view_mapping_list)[0:k]]\n",
    "    return top_k_views\n",
    "\n",
    "def phased_get_top_k_utility_views(kl_divergence_view_mapping, k):\n",
    "    sorted_items = sorted(kl_divergence_view_mapping.items(), key=lambda kv: np.mean(kv[1]), reverse=True)[0:k]\n",
    "    return [item[0] for item in sorted_items]\n",
    "\n",
    "def get_phase_partitions(total_rows, phases):\n",
    "    return [total_rows // phases + (1 if x < total_rows % phases else 0) for x in range (phases)]\n",
    "\n",
    "def get_skewed_phase_partitions(total_rows, phases, beta=1):\n",
    "    # y = alpha*x^beta\n",
    "    alpha=0.01\n",
    "    x = (total_rows / alpha)**(1/beta)\n",
    "    partitions = []\n",
    "    start = 0\n",
    "    for i in range(phases):\n",
    "        end = math.ceil(alpha*((((i+1)*x)/phases)**beta))\n",
    "        partitions.append(end - start)\n",
    "        start = end\n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-internet",
   "metadata": {},
   "source": [
    "## Getting user input and setting the target_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "spread-stockholm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the attribute and associative value for the where clause seperated by space for target_dataset:  marital_status Married\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target dataset create command: create table target_dataset as select * from adult where marital_status = 'Married';\n",
      "Creation of target table successfull.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the attribute and associative value for the where clause seperated by space for reference_dataset:  marital_status Unmarried\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target dataset create command: create table reference_dataset as select * from adult where marital_status = 'Unmarried';\n",
      "Creation of reference table successfull.\n",
      "The reference dataset is reference_dataset\n"
     ]
    }
   ],
   "source": [
    "#test_query = select * from adult where relationship =' Unmarried'; \n",
    "try:\n",
    "    attribute_value_input = input(\"Enter the attribute and associative value for the where clause seperated by space for target_dataset: \")\n",
    "    target_attribute, target_value = attribute_value_input.split(\" \")\n",
    "    query = f\"select * from adult where {target_attribute} = '{target_value}'\" \n",
    "    #cur = get_cursor(conn)\n",
    "    cur.execute(f\"\"\"DROP table IF EXISTS {target_dataset};\"\"\")\n",
    "    print(f\"\"\"Target dataset create command: create table {target_dataset} as {query};\"\"\")\n",
    "    cur.execute(f\"\"\"create table {target_dataset} as {query};\"\"\")\n",
    "    conn.commit()\n",
    "    print(f\"Creation of target table successfull.\")\n",
    "    \n",
    "    attribute_value_input = input(\"Enter the attribute and associative value for the where clause seperated by space for reference_dataset: \")\n",
    "    reference_attribute, reference_value = attribute_value_input.split(\" \")\n",
    "    query = f\"select * from adult where {reference_attribute} = '{reference_value}'\" \n",
    "    #cur = get_cursor(conn)\n",
    "    cur.execute(f\"\"\"DROP table IF EXISTS {reference_dataset};\"\"\")\n",
    "    print(f\"\"\"Target dataset create command: create table {reference_dataset} as {query};\"\"\")\n",
    "    cur.execute(f\"\"\"create table {reference_dataset} as {query};\"\"\")\n",
    "    conn.commit()\n",
    "    print(f\"Creation of reference table successfull.\")\n",
    "except Exception as err:\n",
    "    print(\"Error in establishing target db\")\n",
    "    print_psycopg2_exception(err)\n",
    "    conn = None\n",
    "    \n",
    "print(f\"The reference dataset is {reference_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-inspector",
   "metadata": {},
   "source": [
    "## Get all views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "operating-aurora",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total views at start for consideration 96\n"
     ]
    }
   ],
   "source": [
    "views = get_all_views(groupby_attributes, measure_attributes, aggregate_functions)\n",
    "print(\"Total views at start for consideration\", len(views))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-statement",
   "metadata": {},
   "source": [
    "## Basic Implementation (Don't need to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "municipal-singing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5417311191558838\n",
      "[('workclass', 'capital_loss', 'max'), ('salary', 'capital_gain', 'avg'), ('salary', 'capital_loss', 'max'), ('salary', 'hours_per_week', 'min'), ('native_country', 'capital_loss', 'min')]\n"
     ]
    }
   ],
   "source": [
    "def basic_implementation(views):\n",
    "    kl_divergence_view_mapping_list = []\n",
    "    for groupby_attribute, measure_attribute, aggregate_function in views:\n",
    "        target_query, reference_query = basic_query_generator(groupby_attribute, measure_attribute, aggregate_function, target_dataset, reference_dataset)\n",
    "        target_result = execute_get_query(get_cursor(conn), target_query)\n",
    "        reference_result = execute_get_query(get_cursor(conn), reference_query)\n",
    "        target_values, reference_values = get_key_based_values(target_result, reference_result)\n",
    "        kl_divergence = calculate_kl_divergence(target_values, reference_values)\n",
    "        kl_divergence_view_mapping_list.append((kl_divergence, (groupby_attribute, measure_attribute, aggregate_function)))\n",
    "    \n",
    "    return kl_divergence_view_mapping_list\n",
    "\n",
    "starttime = time.time()\n",
    "kl_divergence_view_mapping_list = basic_implementation(views)\n",
    "top_k_views = basic_get_top_k_utility_views(kl_divergence_view_mapping_list,  K)\n",
    "print(time.time() - starttime)\n",
    "print(top_k_views)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-jaguar",
   "metadata": {},
   "source": [
    "## Shared Optimization on Basic (Don't need to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "failing-milton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49353885650634766\n",
      "[('workclass', 'capital_loss', 'max'), ('sex', 'age', 'max'), ('sex', 'capital_gain', 'max'), ('sex', 'hours_per_week', 'max'), ('race', 'hours_per_week', 'max')]\n"
     ]
    }
   ],
   "source": [
    "kl_divergence_view_mapping_list = []\n",
    "aggregate_function_measure_attribute_combinations = get_aggregate_function_measure_attribute_combinations(measure_attributes, aggregate_functions)\n",
    "for groupby_attribute in groupby_attributes:\n",
    "    target_query, reference_query = get_combined_aggregate_query(groupby_attribute, measure_attributes, aggregate_functions, target_dataset, reference_dataset)\n",
    "    target_result = execute_get_query(get_cursor(conn), target_query)\n",
    "    reference_result = execute_get_query(get_cursor(conn), reference_query)\n",
    "    aggregate_function_measure_attribute_based_values = get_aggregate_function_measure_attribute_based_values(target_result, reference_result, aggregate_function_measure_attribute_combinations)\n",
    "    aggregate_function_measure_attribute_based_KL_divergence = get_aggregate_function_measure_attribute_based_KL_divergence(aggregate_function_measure_attribute_based_values)\n",
    "    for aggregate_function_measure_attribute_pair in aggregate_function_measure_attribute_based_KL_divergence:\n",
    "        kl_divergence = aggregate_function_measure_attribute_based_KL_divergence[aggregate_function_measure_attribute_pair]\n",
    "        kl_divergence_view_mapping_list.append((kl_divergence, (groupby_attribute, aggregate_function_measure_attribute_pair[0], aggregate_function_measure_attribute_pair[1])))\n",
    "\n",
    "starttime = time.time()      \n",
    "kl_divergence_view_mapping_list = basic_implementation(views)\n",
    "top_k_views = basic_get_top_k_utility_views(kl_divergence_view_mapping_list,  K)\n",
    "print(time.time() - starttime)\n",
    "print(top_k_views)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-coordination",
   "metadata": {},
   "source": [
    "## Phased Implementation with Shared & Pruning Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-medium",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "anticipated-education",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_phases = 10\n",
    "delta = 0.1  # probability bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "abroad-halifax",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Map view->kl divergence list\n",
    "def phased_implementation(views, beta=1):\n",
    "    target_dataset_rows = execute_get_query(get_cursor(conn), f\"\"\"select count(*) from {target_dataset};\"\"\")[0][0]\n",
    "    reference_dataset_rows = execute_get_query(get_cursor(conn), f\"\"\"select count(*) from {reference_dataset};\"\"\")[0][0]\n",
    "    target_dataset_partitions = get_skewed_phase_partitions(target_dataset_rows, num_phases, beta)\n",
    "    reference_dataset_partitions = get_skewed_phase_partitions(reference_dataset_rows, num_phases, beta)\n",
    "\n",
    "    # 13, 33\n",
    "    # 13 -> 5, 4, 4 [1, 2, 3, 4, 5], [6, 7, 8, 9], [10, 11, 12, 13]\n",
    "    # 33 -> 11, 11, 11\n",
    "    target_start = reference_start = 0\n",
    "    kl_divergence_view_mapping = defaultdict(list)\n",
    "    \n",
    "    aggregate_function_measure_attribute_combinations = get_aggregate_function_measure_attribute_combinations(measure_attributes, aggregate_functions)\n",
    "    #print(f\"The combinations are {aggregate_function_measure_attribute_combinations}\")\n",
    "    groupby_attribute_combination_mapping = {}\n",
    "    \n",
    "    for groupby_attribute in groupby_attributes:\n",
    "        groupby_attribute_combination_mapping[groupby_attribute] = set([(combination[0], combination[1]) for combination in aggregate_function_measure_attribute_combinations])\n",
    "    \n",
    "    #print(groupby_attribute_combination_mapping)\n",
    "    \n",
    "    for phase in range(num_phases):\n",
    "        #print(f\"Phase {phase}\")\n",
    "        #print(f\"groupby_attribute_combination_mapping => {groupby_attribute_combination_mapping}\")\n",
    "        \n",
    "        target_offset = target_dataset_partitions[phase]\n",
    "        reference_offset = reference_dataset_partitions[phase]\n",
    "        \n",
    "        for groupby_attribute in groupby_attribute_combination_mapping.keys():\n",
    "            #print(f\"Processing groupby attribute {groupby_attribute}\")\n",
    "            ##For fixing the combination place in the list\n",
    "            measure_attribute_aggregate_function_combinations = list(groupby_attribute_combination_mapping[groupby_attribute])\n",
    "            target_query, reference_query = get_phased_combined_aggregate_query(groupby_attribute, \\\n",
    "                                                                            measure_attribute_aggregate_function_combinations, \\\n",
    "                                                                            target_dataset, \\\n",
    "                                                                            reference_dataset, \\\n",
    "                                                                            [target_start, target_offset], \\\n",
    "                                                                            [reference_start, reference_offset])\n",
    "            #print(f\"target query => {target_query}\")\n",
    "            \n",
    "            target_result = execute_get_query(get_cursor(conn), target_query)\n",
    "            reference_result = execute_get_query(get_cursor(conn), reference_query)\n",
    "            \n",
    "            aggregate_function_measure_attribute_based_values = get_aggregate_function_measure_attribute_based_values(target_result, reference_result, measure_attribute_aggregate_function_combinations)\n",
    "            aggregate_function_measure_attribute_based_KL_divergence = get_aggregate_function_measure_attribute_based_KL_divergence(aggregate_function_measure_attribute_based_values)\n",
    "            \n",
    "            #print(f\"The KL divergence mapping for groupby attribute {groupby_attribute} => {aggregate_function_measure_attribute_based_KL_divergence}\")\n",
    "            \n",
    "            for aggregate_function_measure_attribute_pair in aggregate_function_measure_attribute_based_KL_divergence:\n",
    "                kl_divergence = aggregate_function_measure_attribute_based_KL_divergence[aggregate_function_measure_attribute_pair]\n",
    "                kl_divergence_view_mapping[(groupby_attribute, aggregate_function_measure_attribute_pair[0], aggregate_function_measure_attribute_pair[1])].append(kl_divergence)\n",
    "            \n",
    "        # ignore first phase for view dropping\n",
    "        if phase > 0:\n",
    "            m = phase + 1\n",
    "            N = num_phases                \n",
    "                \n",
    "            # calculation of confidence interval em\n",
    "            numerator = (1-(m-1)/N)*(2*np.log10(np.log10(m))) + np.log10((np.pi)**2/3/delta)\n",
    "            em = np.sqrt(numerator/2/m)\n",
    "\n",
    "            # find top k views sorting by decreasing upper bound\n",
    "            current_view_scores = [(k, v) for k, v in kl_divergence_view_mapping.items()]\n",
    "            views_ranking = sorted(current_view_scores, key=lambda x: np.mean(x[1]), reverse=True)\n",
    "            \n",
    "            # smallest lower bound of top k views\n",
    "            top_k_smallest_lower_bound = np.mean(views_ranking[K][1]) - em            \n",
    "            \n",
    "            # dropping the views\n",
    "            for view in list(kl_divergence_view_mapping.keys()):\n",
    "                if np.mean(kl_divergence_view_mapping[view])+em < top_k_smallest_lower_bound:\n",
    "                    del kl_divergence_view_mapping[view]\n",
    "                    groupby_attribute_combination_mapping[view[0]].remove((view[1], view[2]))\n",
    "                    if len(groupby_attribute_combination_mapping[view[0]]) == 0:\n",
    "                        del groupby_attribute_combination_mapping[view[0]]\n",
    "            \n",
    "        target_start += target_offset\n",
    "        reference_start += reference_offset\n",
    "        \n",
    "    return kl_divergence_view_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-steal",
   "metadata": {},
   "source": [
    "## Final Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "sublime-limitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3390991687774658\n",
      "[('native_country', 'capital_gain', 'avg'), ('education', 'capital_gain', 'avg'), ('native_country', 'capital_loss', 'avg'), ('relationship', 'capital_gain', 'avg'), ('workclass', 'capital_gain', 'avg')]\n",
      "0.21381402015686035\n",
      "[('native_country', 'capital_gain', 'avg'), ('education', 'capital_gain', 'avg'), ('native_country', 'capital_loss', 'avg'), ('race', 'capital_gain', 'avg'), ('native_country', 'capital_gain', 'min')]\n",
      "0.19111967086791992\n",
      "[('native_country', 'capital_gain', 'avg'), ('education', 'capital_gain', 'avg'), ('native_country', 'capital_loss', 'avg'), ('relationship', 'capital_gain', 'avg'), ('native_country', 'capital_gain', 'min')]\n",
      "0.1745009422302246\n",
      "[('native_country', 'capital_gain', 'avg'), ('education', 'capital_gain', 'avg'), ('native_country', 'capital_loss', 'avg'), ('workclass', 'capital_gain', 'avg'), ('native_country', 'capital_loss', 'min')]\n"
     ]
    }
   ],
   "source": [
    "starttime = time.time()\n",
    "beta = 1\n",
    "kl_divergence_view_mapping = phased_implementation(views, beta)\n",
    "top_k_views = phased_get_top_k_utility_views(kl_divergence_view_mapping,  K)\n",
    "print(time.time() - starttime)\n",
    "print(top_k_views)\n",
    "\n",
    "starttime = time.time()\n",
    "beta = 1.3\n",
    "kl_divergence_view_mapping = phased_implementation(views, beta)\n",
    "top_k_views = phased_get_top_k_utility_views(kl_divergence_view_mapping,  K)\n",
    "print(time.time() - starttime)\n",
    "print(top_k_views)\n",
    "\n",
    "starttime = time.time()\n",
    "beta = 1.5\n",
    "kl_divergence_view_mapping = phased_implementation(views, beta)\n",
    "top_k_views = phased_get_top_k_utility_views(kl_divergence_view_mapping,  K)\n",
    "print(time.time() - starttime)\n",
    "print(top_k_views)\n",
    "\n",
    "starttime = time.time()\n",
    "beta = 2\n",
    "kl_divergence_view_mapping = phased_implementation(views, beta)\n",
    "top_k_views = phased_get_top_k_utility_views(kl_divergence_view_mapping,  K)\n",
    "print(time.time() - starttime)\n",
    "print(top_k_views)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-diagram",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-alloy",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (9,5)\n",
    "plt.rcParams['axes.facecolor']='white'\n",
    "\n",
    "for a, m, f in top_k_views:\n",
    "    # get target and reference data\n",
    "    target_query, reference_query = basic_query_generator(a, m, f, target_dataset, reference_dataset)\n",
    "    #print(target_query, reference_query)\n",
    "    target_result = execute_get_query(get_cursor(conn), target_query)\n",
    "    reference_result = execute_get_query(get_cursor(conn), reference_query)\n",
    "    print(a,m,f)\n",
    "\n",
    "    # create plot\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_facecolor((1, 1, 1))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    \n",
    "    bar_width = 0.35\n",
    "    opacity = 0.8\n",
    "\n",
    "    tgt_dict = dict(target_result)\n",
    "    ref_dict = dict(reference_result)\n",
    "\n",
    "    for k in tgt_dict.keys():\n",
    "        if k not in ref_dict:\n",
    "            ref_dict[k] = 0\n",
    "\n",
    "    for k in ref_dict.keys():\n",
    "        if k not in tgt_dict:\n",
    "            tgt_dict[k] = 0\n",
    "\n",
    "\n",
    "    index = np.arange(len(tgt_dict))\n",
    "    rects1 = plt.bar(index, tgt_dict.values(), bar_width,\n",
    "                     alpha=opacity,\n",
    "                     color=(1/255, 111/255, 196/255, 1/255),\n",
    "                     label=target_value)\n",
    "\n",
    "    rects2 = plt.bar(index + bar_width, ref_dict.values(), bar_width,\n",
    "                     alpha=opacity,\n",
    "                     color=(80/255,227/255,194/255,1/255),\n",
    "                     label=reference_value)\n",
    "\n",
    "    plt.xlabel('{}'.format(a))\n",
    "    plt.ylabel('{}({})'.format(f, m))\n",
    "    plt.xticks(index + bar_width, tgt_dict.keys(), rotation=\"vertical\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-canvas",
   "metadata": {},
   "outputs": [],
   "source": [
    "Percentage_of_data = [20,40,60,80,100]\n",
    "Response_time_Basic = [.53,.93,1.38,1.8,2.2]\n",
    "Response_time_with_Sharing_Opt = [.5,.89,1.27,1.7,2.1]\n",
    "Response_time_with_All_Opt= [.09,.12,.18,.2,.224]\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.patch.set_facecolor('white')\n",
    "plt.rcParams[\"figure.figsize\"] = (18,10)\n",
    "plt.plot(Percentage_of_data, Response_time_Basic, color='red', label='Basic', marker='o')\n",
    "plt.plot(Percentage_of_data, Response_time_with_Sharing_Opt, color='blue', label='Basic with Sharing Opt', marker='x')\n",
    "plt.plot(Percentage_of_data, Response_time_with_All_Opt, color='green', label='Phased with All Opt', marker='+')\n",
    "plt.title('Response Time Vs % of Total Data for Different Methods')\n",
    "plt.xlabel('% of Total Data')\n",
    "plt.ylabel('Response Time(s)')\n",
    "leg = plt.legend(loc='upper center')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
